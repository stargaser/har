---
title: "Human Activity Recognition"
author: "David Shupe"
date: "December 27, 2015"
output: html_document
---

## Executive Summary

This report concerns human activity recognition, for a dataset in which various
"fitness tracker"-style measurements were recorded while study participants performed
weightlifting exercises with good form and with poor form. The training set included
5 classes for the exercises. We removed features containing mostly NA values, as well
as several features that were simply identification information (like participant
name and time the data were taken). We trained both a CART model and a random 
forest model. The simple CART model reached only a 50% accuracy on the training set
but the random forest model reached 99.5% accuracy on the same set. The random
forest model predicted all 20 test cases correctly.

## Data Input

```{r, echo=FALSE, warning=FALSE}
setwd('/Users/shupe/Documents/DataScienceJHU/RWork/machine_learning/project/')
library(caret)
library(randomForest)
```

The training set was provided in 'pml-training.csv' and the test set in
'pml-testing.csv'. After some iteration, we read in the values treating
'NA', blank, or the Excel-ism '#DIV/0!' as missing values.

```{r, echo=FALSE}
training <- read.csv('pml-training.csv', na.strings=c(" ","#DIV/0!","NA"))
test <- read.csv('pml-testing.csv',na.strings=c(" ","#DIV/0!","NA"))
training$classe <- as.factor(training$classe)
```

A summary of the training set shows many of the 160 columns are populated mostly
with NAs.
```{r}
summary(training)
```

We remove the columns that have more than 1000 NAs. This reduces the number of
columns from 160 to 60. Then we remove the first 7 columns. These contain
identifying information such as an index, the name of the participant, timestamps
of when the data were recorded, and whether the row is the start of a time window.
We'd like our model to apply regardless of participant or when the data were taken,
so our intuition is to remove these features, leaving us only with the
"fitness tracker"-style measurements.


```{r}
good_training <- training[,colSums(is.na(training)) < 1000]
good_training <- good_training[,8:ncol(good_training)]
na_count <- data.frame(sapply(good_training, function(y) sum(length(which(is.na(y))))))
na_count
```

We are left with no NA values, so we won't have to impute missing values
in the pre-processing set.

```{r}
summary(good_training)
```

We apply centering and scaling to the training features. The preprocessing
steps are saved for later application to the test set.
```{r}
trainX <- good_training[,names(good_training) != 'classe']
preProcValues <- preProcess(trainX, method = c("center", "scale"))
scaledTrain <- predict(preProcValues, trainX)
scaledTrain$classe <- good_training$classe
```

## CART model

First we fit a CART model. Following the caret tutorial by Max Kuhn,
we use 10-fold cross-validation with 3 repeats.

```{r}
cvCtrl <- trainControl(method = "repeatedcv", repeats = 3)
set.seed(33833)
rpartTune <- train(classe ~ ., method = "rpart", data=scaledTrain,
                    trControl = cvCtrl)

```

How did we do?
```{r}
rpartTune
```
Wow, 50% accuracy is pretty lousy.

What are the most important variables?
```{r}
varImp(rpartTune)
```

We can see that many variables are not used in the CART model. In some sense
it is not digging deeply into the data, in terms of numbers of features used.

## Random forest model

Given the richness of the dataset and the small number of classes to predict,
a random forest model is likely to do an excellent job. Here again we use
10-fold cross-validation with 3 repeats, as recommended by Max Kuhn in
the caret tutorial.

```{r, cache=TRUE}
set.seed(33833)
cvCtrl <- trainControl(method = "repeatedcv", repeats = 3)
rfFit <- train(classe ~ ., method="rf", data=scaledTrain,  
               trControl = cvCtrl)
```

How did we do?
```{r}
rfFit
```

An accuracy of 99.5% is quite good!

What are the most important variables?
```{r}
varImp(rfFit)
```

We can see that many more features are used by the random forest model than
by the CART model.

## Applying the random forest model to the test set

For the model to properly predict the "classe" for the test data, it
is first necessary to apply the same preprocessing to the test set that
was done to the training set. We remove the "NA" columns and the first
7 columns with number, name, timestamp and window information.

```{r}
good_test <- test[,colSums(is.na(training)) ==0]
ncol(good_test)
good_test <- good_test[,8:ncol(good_test)]
```

We remove the "problem_id" column, apply the centering and scaling
preprocessing done for the training set, and add back the "problem_id" column.

```{r}
testX <- good_test[,names(good_test) != 'problem_id']
scaledTest <- predict(preProcValues, testX)
scaledTest$problem_id <- good_test$problem_id
```

Now we are ready to apply our model to the test set.

```{r}
predTest <- predict(rfFit,newdata=scaledTest)
answers <- as.character(predTest)
```

## Scoring on the test set

Following the instructions in the assignment, we write text files for each
answer for submission to the online automatic grader.

```{r, echo=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
setwd('./answers')
pml_write_files(answers)
```

The autograder's feedback is that every prediction is correct! The random
forest model proves to be very accurate, even though it takes some time to compute.

## References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
http://groupware.les.inf.puc-rio.br/har#ixzz3v17KTE2Y

"Predictive Modeling with R and the caret Package," Max Kuhn, Ph.D,
Pfizer Global R&D Groton, CT. max.kuhn@pfizer.com.
http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
